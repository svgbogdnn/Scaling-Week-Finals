{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "072001c9",
   "metadata": {
    "papermill": {
     "duration": 0.002108,
     "end_time": "2025-11-18T01:21:54.183311",
     "exception": false,
     "start_time": "2025-11-18T01:21:54.181203",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## Problem statement\n",
    "\n",
    "Your task is to speed up the `forward pass` of the `swiglu` function:\n",
    "```python\n",
    "def swiglu(a, b):\n",
    "    return torch.nn.functional.silu(a) * b\n",
    "````\n",
    "\n",
    "Your implementation must be written in `Triton`, be called via the interface\n",
    "`torch.ops.llm_scaling_week.swiglu_fwd(a, b)` and return only a single\n",
    "`torch.Tensor` – the result of the operation.\n",
    "\n",
    "The implementation will be checked for correctness and performance.\n",
    "To pass the correctness test, the result of your function must match the output of\n",
    "the eager `swiglu` implementation under `torch.allclose`.\n",
    "To pass the performance test, your function must take **≤ 75%** of the time of the\n",
    "eager `swiglu` implementation.\n",
    "\n",
    "**It is guaranteed that the inputs will be `contiguous` tensors. The types and shapes\n",
    "of the input tensors `a` and `b` are the same.**\n",
    "Note that the function must work with both `fp32` and `bf16` tensors.\n",
    "The function must work efficiently regardless of the tensor shape.\n",
    "\n",
    "The reference solution passes all tests both on H100 and in Google Colab.\n",
    "\n",
    "## Note\n",
    "\n",
    "You can view the test logs by downloading the output of test 1 on the contest website.\n",
    "Do not rename the `solution.py` file. Your solution must be in this file.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "bcd619a2",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-11-18T01:21:54.187570Z",
     "iopub.status.busy": "2025-11-18T01:21:54.187308Z",
     "iopub.status.idle": "2025-11-18T01:22:02.471157Z",
     "shell.execute_reply": "2025-11-18T01:22:02.470298Z"
    },
    "papermill": {
     "duration": 8.287469,
     "end_time": "2025-11-18T01:22:02.472523",
     "exception": false,
     "start_time": "2025-11-18T01:21:54.185054",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CUDA available: True\n"
     ]
    }
   ],
   "source": [
    "!pip install -q triton\n",
    "\n",
    "import torch\n",
    "print(\"CUDA available:\", torch.cuda.is_available())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "dd64a5fa",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-11-18T01:22:02.477102Z",
     "iopub.status.busy": "2025-11-18T01:22:02.476788Z",
     "iopub.status.idle": "2025-11-18T01:22:03.778097Z",
     "shell.execute_reply": "2025-11-18T01:22:03.777500Z"
    },
    "papermill": {
     "duration": 1.305376,
     "end_time": "2025-11-18T01:22:03.779533",
     "exception": false,
     "start_time": "2025-11-18T01:22:02.474157",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# First try\n",
    "\n",
    "# import torch\n",
    "# import triton\n",
    "# import triton.language as tl\n",
    "# from torch.library import Library\n",
    "\n",
    "# _lib = Library(\"llm_scaling_week\", \"DEF\")\n",
    "# _lib.define(\"swiglu_fwd(Tensor a, Tensor b) -> Tensor\") \n",
    "\n",
    "# @triton.jit\n",
    "# def swiglu_kernel(a_ptr, b_ptr, out_ptr, n_elements: tl.constexpr, BLOCK_SIZE: tl.constexpr):\n",
    "#     pid = tl.program_id(axis=0)                       \n",
    "#     offs = pid * BLOCK_SIZE + tl.arange(0, BLOCK_SIZE) \n",
    "#     mask = offs < n_elements                           \n",
    "    \n",
    "#     a_vals = tl.load(a_ptr + offs, mask=mask, other=0.0)\n",
    "#     b_vals = tl.load(b_ptr + offs, mask=mask, other=0.0)\n",
    "    \n",
    "#     a_vals_f32 = a_vals.to(tl.float32)\n",
    "#     b_vals_f32 = b_vals.to(tl.float32)\n",
    "#     silu_a = a_vals_f32 * tl.sigmoid(a_vals_f32)\n",
    "#     out_f32 = silu_a * b_vals_f32\n",
    "    \n",
    "#     out_vals = out_f32.to(a_vals.dtype)  \n",
    "#     tl.store(out_ptr + offs, out_vals, mask=mask)     \n",
    "    \n",
    "\n",
    "# def _swiglu_fwd_impl(a: torch.Tensor, b: torch.Tensor) -> torch.Tensor:\n",
    "#     if not a.is_cuda or not b.is_cuda:\n",
    "#         raise RuntimeError(\"swiglu_fwd: both inputs must be CUDA tensors\")\n",
    "#     if a.dtype not in (torch.float32, torch.bfloat16) or b.dtype not in (torch.float32, torch.bfloat16):\n",
    "#         raise RuntimeError(\"swiglu_fwd: inputs must be float32 or bfloat16 tensors\")\n",
    "#     if a.dtype != b.dtype:\n",
    "#         raise RuntimeError(\"swiglu_fwd: inputs must have the same dtype\")\n",
    "#     if a.shape != b.shape:\n",
    "#         raise RuntimeError(\"swiglu_fwd: inputs must have the same shape\")\n",
    "#     if not a.is_contiguous():\n",
    "#         a = a.contiguous()\n",
    "#     if not b.is_contiguous():\n",
    "#         b = b.contiguous()\n",
    "#     out = torch.empty_like(a)\n",
    "    \n",
    "#     n_elements = a.numel()\n",
    "#     grid = (triton.cdiv(n_elements, 256),) \n",
    "#     swiglu_kernel[grid](a, b, out, n_elements, BLOCK_SIZE=256, num_warps=4)\n",
    "#     return out\n",
    "\n",
    "# _impl_lib = Library(\"llm_scaling_week\", \"IMPL\")\n",
    "# _impl_lib.impl(\"swiglu_fwd\", _swiglu_fwd_impl, \"CUDA\")\n",
    "\n",
    "\n",
    "import torch\n",
    "import triton\n",
    "import triton.language as tl\n",
    "from torch.library import Library\n",
    "\n",
    "try:\n",
    "    _def_lib = Library(\"llm_scaling_week\", \"DEF\")\n",
    "    _def_lib.define(\"swiglu_fwd(Tensor a, Tensor b) -> Tensor\")\n",
    "except RuntimeError:\n",
    "    pass\n",
    "\n",
    "\n",
    "@triton.autotune(\n",
    "    configs=[\n",
    "        triton.Config({\"BLOCK_SIZE\": 256}, num_warps=4),\n",
    "        triton.Config({\"BLOCK_SIZE\": 512}, num_warps=4),\n",
    "        triton.Config({\"BLOCK_SIZE\": 1024}, num_warps=8),\n",
    "        triton.Config({\"BLOCK_SIZE\": 2048}, num_warps=8),\n",
    "    ],\n",
    "    key=[\"n_elements\"],\n",
    ")\n",
    "@triton.jit\n",
    "def swiglu_kernel(a_ptr, b_ptr, out_ptr, n_elements, BLOCK_SIZE: tl.constexpr):\n",
    "    pid = tl.program_id(0)\n",
    "    offs = pid * BLOCK_SIZE + tl.arange(0, BLOCK_SIZE)\n",
    "    mask = offs < n_elements\n",
    "\n",
    "    a_vals = tl.load(a_ptr + offs, mask=mask, other=0.0)\n",
    "    b_vals = tl.load(b_ptr + offs, mask=mask, other=0.0)\n",
    "\n",
    "    a_f32 = a_vals.to(tl.float32)\n",
    "    b_f32 = b_vals.to(tl.float32)\n",
    "\n",
    "    silu = a_f32 * tl.sigmoid(a_f32)\n",
    "    out_f32 = silu * b_f32\n",
    "\n",
    "    out_vals = out_f32.to(a_vals.dtype)\n",
    "    tl.store(out_ptr + offs, out_vals, mask=mask)\n",
    "\n",
    "\n",
    "def _swiglu_fwd_cuda(a: torch.Tensor, b: torch.Tensor) -> torch.Tensor:\n",
    "    if not (a.is_cuda and b.is_cuda):\n",
    "        raise RuntimeError(\"swiglu_fwd: CUDA only\")\n",
    "    if a.dtype != b.dtype:\n",
    "        raise RuntimeError(\"swiglu_fwd: dtypes must match\")\n",
    "    if a.shape != b.shape:\n",
    "        raise RuntimeError(\"swiglu_fwd: shapes must match\")\n",
    "    if a.dtype not in (torch.float32, torch.bfloat16):\n",
    "        raise RuntimeError(\"swiglu_fwd: supports only float32 and bfloat16\")\n",
    "\n",
    "    if not a.is_contiguous():\n",
    "        a = a.contiguous()\n",
    "    if not b.is_contiguous():\n",
    "        b = b.contiguous()\n",
    "\n",
    "    out = torch.empty_like(a)\n",
    "    n_elements = a.numel()\n",
    "\n",
    "    grid = lambda meta: (triton.cdiv(n_elements, meta[\"BLOCK_SIZE\"]),)\n",
    "    swiglu_kernel[grid](a, b, out, n_elements)\n",
    "\n",
    "    return out\n",
    "\n",
    "\n",
    "def _swiglu_fwd_cpu(a: torch.Tensor, b: torch.Tensor) -> torch.Tensor:\n",
    "    return torch.nn.functional.silu(a) * b\n",
    "\n",
    "\n",
    "_impl_lib = Library(\"llm_scaling_week\", \"IMPL\")\n",
    "_impl_lib.impl(\"swiglu_fwd\", _swiglu_fwd_cuda, \"CUDA\")\n",
    "_impl_lib.impl(\"swiglu_fwd\", _swiglu_fwd_cpu, \"CPU\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "836722e8",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-11-18T01:22:03.783814Z",
     "iopub.status.busy": "2025-11-18T01:22:03.783588Z",
     "iopub.status.idle": "2025-11-18T01:22:08.209871Z",
     "shell.execute_reply": "2025-11-18T01:22:08.208938Z"
    },
    "papermill": {
     "duration": 4.429809,
     "end_time": "2025-11-18T01:22:08.211104",
     "exception": false,
     "start_time": "2025-11-18T01:22:03.781295",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.float32 allclose = True\n"
     ]
    }
   ],
   "source": [
    "device = \"cuda\"\n",
    "\n",
    "for dtype in (torch.float32,): \n",
    "    a = torch.randn(4096, 4096, device=device, dtype=dtype)\n",
    "    b = torch.randn_like(a)\n",
    "\n",
    "    ref = torch.nn.functional.silu(a) * b\n",
    "    out = torch.ops.llm_scaling_week.swiglu_fwd(a, b)\n",
    "\n",
    "    print(dtype, \"allclose =\", torch.allclose(ref, out, atol=1e-3, rtol=1e-3))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "ae33b747",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-11-18T01:22:08.216074Z",
     "iopub.status.busy": "2025-11-18T01:22:08.215843Z",
     "iopub.status.idle": "2025-11-18T01:22:09.221270Z",
     "shell.execute_reply": "2025-11-18T01:22:09.220331Z"
    },
    "papermill": {
     "duration": 1.009356,
     "end_time": "2025-11-18T01:22:09.222546",
     "exception": false,
     "start_time": "2025-11-18T01:22:08.213190",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "eager : 0.2787649631500244\n",
      "ops   : 0.15988707542419434\n",
      "ratio : 0.573555132673352\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "def bench(fn, iters=100):\n",
    "    torch.cuda.synchronize()\n",
    "    t0 = time.time()\n",
    "    for _ in range(iters):\n",
    "        fn()\n",
    "    torch.cuda.synchronize()\n",
    "    return (t0, time.time() - t0)\n",
    "\n",
    "a = torch.randn(8192, 4096, device=device, dtype=torch.float32)\n",
    "b = torch.randn_like(a)\n",
    "\n",
    "for _ in range(10):\n",
    "    _ = torch.nn.functional.silu(a) * b\n",
    "    _ = torch.ops.llm_scaling_week.swiglu_fwd(a, b)\n",
    "\n",
    "_, t_eager = bench(lambda: torch.nn.functional.silu(a) * b)\n",
    "_, t_triton = bench(lambda: torch.ops.llm_scaling_week.swiglu_fwd(a, b))\n",
    "\n",
    "print(\"eager :\", t_eager)\n",
    "print(\"ops   :\", t_triton)\n",
    "print(\"ratio :\", t_triton / t_eager)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "a53a8148",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-11-18T01:22:09.227720Z",
     "iopub.status.busy": "2025-11-18T01:22:09.227114Z",
     "iopub.status.idle": "2025-11-18T01:22:09.232989Z",
     "shell.execute_reply": "2025-11-18T01:22:09.232472Z"
    },
    "papermill": {
     "duration": 0.009519,
     "end_time": "2025-11-18T01:22:09.234083",
     "exception": false,
     "start_time": "2025-11-18T01:22:09.224564",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"\\n============================= test session starts ==============================\\nplatform linux -- Python 3.11.14, pytest-9.0.0, pluggy-1.6.0 -- /opt/conda/bin/python3.11\\ncachedir: .pytest_cache\\nhypothesis profile 'default'\\nrootdir: /workspace\\nplugins: hypothesis-6.141.0\\ncollecting ... collected 16 items\\n\\ntests.py::test_swiglu_fwd_quality_check[shape0-dtype0] PASSED            [  6%]\\ntests.py::test_swiglu_fwd_quality_check[shape0-dtype1] PASSED            [ 12%]\\ntests.py::test_swiglu_fwd_quality_check[shape1-dtype0] PASSED            [ 18%]\\ntests.py::test_swiglu_fwd_quality_check[shape1-dtype1] PASSED            [ 25%]\\ntests.py::test_swiglu_fwd_quality_check[shape2-dtype0] PASSED            [ 31%]\\ntests.py::test_swiglu_fwd_quality_check[shape2-dtype1] PASSED            [ 37%]\\ntests.py::test_swiglu_fwd_quality_check[shape3-dtype0] PASSED            [ 43%]\\ntests.py::test_swiglu_fwd_quality_check[shape3-dtype1] PASSED            [ 50%]\\ntests.py::test_swiglu_fwd_speed_vs_eager_check[shape0-dtype0] PASSED     [ 56%]\\ntests.py::test_swiglu_fwd_speed_vs_eager_check[shape0-dtype1] PASSED     [ 62%]\\ntests.py::test_swiglu_fwd_speed_vs_eager_check[shape1-dtype0] PASSED     [ 68%]\\ntests.py::test_swiglu_fwd_speed_vs_eager_check[shape1-dtype1] PASSED     [ 75%]\\ntests.py::test_swiglu_fwd_speed_vs_eager_check[shape2-dtype0] PASSED     [ 81%]\\ntests.py::test_swiglu_fwd_speed_vs_eager_check[shape2-dtype1] PASSED     [ 87%]\\ntests.py::test_swiglu_fwd_speed_vs_eager_check[shape3-dtype0] PASSED     [ 93%]\\ntests.py::test_swiglu_fwd_speed_vs_eager_check[shape3-dtype1] PASSED     [100%]\\n\\n============================== 16 passed in 8.57s ==============================\\n\""
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Output\n",
    "\n",
    "'''\n",
    "============================= test session starts ==============================\n",
    "platform linux -- Python 3.11.14, pytest-9.0.0, pluggy-1.6.0 -- /opt/conda/bin/python3.11\n",
    "cachedir: .pytest_cache\n",
    "hypothesis profile 'default'\n",
    "rootdir: /workspace\n",
    "plugins: hypothesis-6.141.0\n",
    "collecting ... collected 16 items\n",
    "\n",
    "tests.py::test_swiglu_fwd_quality_check[shape0-dtype0] PASSED            [  6%]\n",
    "tests.py::test_swiglu_fwd_quality_check[shape0-dtype1] PASSED            [ 12%]\n",
    "tests.py::test_swiglu_fwd_quality_check[shape1-dtype0] PASSED            [ 18%]\n",
    "tests.py::test_swiglu_fwd_quality_check[shape1-dtype1] PASSED            [ 25%]\n",
    "tests.py::test_swiglu_fwd_quality_check[shape2-dtype0] PASSED            [ 31%]\n",
    "tests.py::test_swiglu_fwd_quality_check[shape2-dtype1] PASSED            [ 37%]\n",
    "tests.py::test_swiglu_fwd_quality_check[shape3-dtype0] PASSED            [ 43%]\n",
    "tests.py::test_swiglu_fwd_quality_check[shape3-dtype1] PASSED            [ 50%]\n",
    "tests.py::test_swiglu_fwd_speed_vs_eager_check[shape0-dtype0] PASSED     [ 56%]\n",
    "tests.py::test_swiglu_fwd_speed_vs_eager_check[shape0-dtype1] PASSED     [ 62%]\n",
    "tests.py::test_swiglu_fwd_speed_vs_eager_check[shape1-dtype0] PASSED     [ 68%]\n",
    "tests.py::test_swiglu_fwd_speed_vs_eager_check[shape1-dtype1] PASSED     [ 75%]\n",
    "tests.py::test_swiglu_fwd_speed_vs_eager_check[shape2-dtype0] PASSED     [ 81%]\n",
    "tests.py::test_swiglu_fwd_speed_vs_eager_check[shape2-dtype1] PASSED     [ 87%]\n",
    "tests.py::test_swiglu_fwd_speed_vs_eager_check[shape3-dtype0] PASSED     [ 93%]\n",
    "tests.py::test_swiglu_fwd_speed_vs_eager_check[shape3-dtype1] PASSED     [100%]\n",
    "\n",
    "============================== 16 passed in 8.57s ==============================\n",
    "'''"
   ]
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "nvidiaTeslaT4",
   "dataSources": [],
   "dockerImageVersionId": 31193,
   "isGpuEnabled": true,
   "isInternetEnabled": true,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  },
  "papermill": {
   "default_parameters": {},
   "duration": 21.026338,
   "end_time": "2025-11-18T01:22:11.345577",
   "environment_variables": {},
   "exception": null,
   "input_path": "__notebook__.ipynb",
   "output_path": "__notebook__.ipynb",
   "parameters": {},
   "start_time": "2025-11-18T01:21:50.319239",
   "version": "2.6.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
