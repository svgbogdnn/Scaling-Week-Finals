{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "4e8e3748",
   "metadata": {
    "papermill": {
     "duration": 0.002451,
     "end_time": "2025-11-19T00:47:50.345061",
     "exception": false,
     "start_time": "2025-11-19T00:47:50.342610",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# Ваша задача - написать эффективную имплементацию операцию `padded_moe_permute`.\n",
    "## (Best speed and quality results of 137 people)\n",
    "Ваша функция должна называться `submission` и иметь следующую сигнатуру:\n",
    "\n",
    "```\n",
    "def submission(\n",
    "    x: torch.Tensor,  # (num_tokens, hidden_size) - входной тензор токенов, каждый размерности hidden_size\n",
    "    top_experts: torch.Tensor,  # (num_tokens, topk) - для каждого токена указано topk экспертов, которые он активирует\n",
    "    tokens_per_expert: torch.Tensor, # (num_experts,) - тензор размерности числа экспертов, i-ый элемент - сколько токенов приходит в i-ого эксперта\n",
    "    topk: int,  # сколько экспертов активируются на каждый токен, например, 8\n",
    "    num_experts: int,  # сколько всего экспертов в MoE, например, 128\n",
    ") -> tuple[\n",
    "    torch.Tensor,  # (max_padded, hidden_size) - padded_tokens, результат пермьюта с паддингами\n",
    "    torch.Tensor. # (num_experts,) - padded_tokens_per_expert, сколько токенов приходят в каждого эксперта вместе с паддингами\n",
    "]\n",
    "```\n",
    "\n",
    "## Для начала рассмотрим стандартную функцию moe_permute без учета паддингов:\n",
    "\n",
    "На вход permute-функции приходит тензор размерности (`num_tokens`, `hidden_size`). Обычно в MoE пермьют переставляет токены так, чтобы токены, попадающие в одного эксперта, находились друг за другом.\n",
    "Например, путь на вход подается\n",
    "```\n",
    "x = tensor([[-0.0236, -0.5368, -0.5663],\n",
    "            [ 0.7778, -0.8583, -0.1123],\n",
    "            [ 0.1981, -0.3514, -0.9443],\n",
    "            [-2.0655, -0.9424,  0.9870]])\n",
    "top_experts = tensor([[1, 3],  # токен 0 выбирает экспертов 1 и 3\n",
    "                    [2, 5],    # токен 1 выбирает 2 и 5\n",
    "                    [3, 5],    # токен 2 выбирает 3 и 5\n",
    "                    [2, 4]])   # токен 3 выбирает 2 и 4\n",
    "```\n",
    "В данном случае `topk=2`, каждый токен выбирает 2 экспертов.\n",
    "Выходной тензор будет иметь размерность `(num_tokens * topk, hidden_size)`, там сначала будут записаны токены для 0ого эксперта, потом для 1ого, потом для 2ого и так далее.\n",
    "В данном случае:\n",
    "```\n",
    "out = tensor([[-0.0236, -0.5368, -0.5663],# токен 0 -> эксперт 1\n",
    "            [ 0.7778, -0.8583, -0.1123],  # токен 1 -> эксперт 2\n",
    "            [-2.0655, -0.9424,  0.9870],  # токен 3 -> эксперт 2\n",
    "            [-0.0236, -0.5368, -0.5663],  # токен 0 -> эксперт 3\n",
    "            [ 0.1981, -0.3514, -0.9443],  # токен 2 -> эксперт 3\n",
    "            [-2.0655, -0.9424,  0.9870],  # токен 3 -> эксперт 4\n",
    "            [ 0.7778, -0.8583, -0.1123],  # токен 1 -> эксперт 5\n",
    "            [ 0.1981, -0.3514, -0.9443]]) # токен 2 -> эксперт 5\n",
    "```\n",
    "\n",
    "**Тензор размерности (num_experts,), который показывает, сколько токенов идут в каждого эксперта, назовем батч сайзами.** В примере выше батч сайзы - это `[1, 2, 2, 1, 2]`.\n",
    "\n",
    "\n",
    "## Теперь к нашей задаче\n",
    "\n",
    "При использовании FP8-умножения из `DeepGEMM` (и других современных кернелов) часто ожидается `TMA`-алаймент тензора, то есть появляется требование делимости размерностей на 128.\n",
    "Это нужно для использования `Tensor Memory Accelerator`-а на H100 для асинхронного копирования из памяти.\n",
    "\n",
    "В случае `moe_permute` это означает необходимость делимости батч сайзов на 128, то есть чтобы в каждого эксперта приходило делящееся на 128 число токенов.\n",
    "В примере выше батч сайзы `[1, 2, 2, 1, 2]` станут `[128, 128, 128, 128, 128]`. А, например, `[128, 1, 129]` перейдет в `[128, 128, 256]`.\n",
    "\n",
    "Чтобы добиться такой гарантии, нам придется западдить результат пермьюта. Теперь он не обязательно будет иметь размерность `num_tokens * topk`, а может содержать дополнительные нулевые токены.\n",
    "Ваша задача - написать функцию, которая будет делать то же самое, что и обычный `moe_permute`, но уже с паддингами - то есть дополнительно будет гарантировать, что первый токен для каждого эксперта начинается с индекса, делящегося на 128.\n",
    "\n",
    "В случае входа из примера выше на выходе мы должны получить\n",
    "```\n",
    "tensor([[-0.0236, -0.5368, -0.5663], # токен 0 -> эксперт 1\n",
    "        [ 0.0000,  0.0000,  0.0000],\n",
    "        [ 0.0000,  0.0000,  0.0000],\n",
    "        ...,\n",
    "        [ 0.7778, -0.8583, -0.1123],  # токен 1 -> эксперт 2, индекс 128\n",
    "        [-2.0655, -0.9424,  0.9870],  # токен 3 -> эксперт 2, индекс 129\n",
    "        [ 0.0000,  0.0000,  0.0000],\n",
    "        [ 0.0000,  0.0000,  0.0000],\n",
    "        [ 0.0000,  0.0000,  0.0000]\n",
    "        ...,\n",
    "        [ 0.0000,  0.0000,  0.0000]])\n",
    "```\n",
    "\n",
    "Для имплементация можно использовать как `Triton`, так и обычный `Torch`.\n",
    "\n",
    "Имплементация будет проверяться на корректность и производительность. Для прохождения теста на корректность результат вашей функций должен совпадать на `torch.allclose` с выходом eager-имплементации.\n",
    "Для прохождения теста на производительность ваша функция должна выдавать скорость примерно совпадающую с нашей референсной имлпементацией. Наша референсная имплементация не очень эффективная, поэтому не спешите сразу начинать с `Triton`.\n",
    "\n",
    "Для простоты вам также дан неэффективный код с `for`-ами.\n",
    "\n",
    "## Примечание\n",
    "Логи тестирования можно посмотреть, скачав вывод в тесте 1 на сайте контеста.\n",
    "Не переименовывайте файл `solution.py`. Ваше решение должно быть в этом файле."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "5faf4e48",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-11-19T00:47:50.349937Z",
     "iopub.status.busy": "2025-11-19T00:47:50.349687Z",
     "iopub.status.idle": "2025-11-19T00:49:08.269508Z",
     "shell.execute_reply": "2025-11-19T00:49:08.268614Z"
    },
    "papermill": {
     "duration": 77.924609,
     "end_time": "2025-11-19T00:49:08.271697",
     "exception": false,
     "start_time": "2025-11-19T00:47:50.347088",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: torch in /usr/local/lib/python3.11/dist-packages (2.6.0+cu124)\r\n",
      "Requirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from torch) (3.20.0)\r\n",
      "Requirement already satisfied: typing-extensions>=4.10.0 in /usr/local/lib/python3.11/dist-packages (from torch) (4.15.0)\r\n",
      "Requirement already satisfied: networkx in /usr/local/lib/python3.11/dist-packages (from torch) (3.5)\r\n",
      "Requirement already satisfied: jinja2 in /usr/local/lib/python3.11/dist-packages (from torch) (3.1.6)\r\n",
      "Requirement already satisfied: fsspec in /usr/local/lib/python3.11/dist-packages (from torch) (2025.10.0)\r\n",
      "Collecting nvidia-cuda-nvrtc-cu12==12.4.127 (from torch)\r\n",
      "  Downloading nvidia_cuda_nvrtc_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\r\n",
      "Collecting nvidia-cuda-runtime-cu12==12.4.127 (from torch)\r\n",
      "  Downloading nvidia_cuda_runtime_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\r\n",
      "Collecting nvidia-cuda-cupti-cu12==12.4.127 (from torch)\r\n",
      "  Downloading nvidia_cuda_cupti_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\r\n",
      "Collecting nvidia-cudnn-cu12==9.1.0.70 (from torch)\r\n",
      "  Downloading nvidia_cudnn_cu12-9.1.0.70-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\r\n",
      "Collecting nvidia-cublas-cu12==12.4.5.8 (from torch)\r\n",
      "  Downloading nvidia_cublas_cu12-12.4.5.8-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\r\n",
      "Collecting nvidia-cufft-cu12==11.2.1.3 (from torch)\r\n",
      "  Downloading nvidia_cufft_cu12-11.2.1.3-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\r\n",
      "Collecting nvidia-curand-cu12==10.3.5.147 (from torch)\r\n",
      "  Downloading nvidia_curand_cu12-10.3.5.147-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\r\n",
      "Collecting nvidia-cusolver-cu12==11.6.1.9 (from torch)\r\n",
      "  Downloading nvidia_cusolver_cu12-11.6.1.9-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\r\n",
      "Collecting nvidia-cusparse-cu12==12.3.1.170 (from torch)\r\n",
      "  Downloading nvidia_cusparse_cu12-12.3.1.170-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\r\n",
      "Requirement already satisfied: nvidia-cusparselt-cu12==0.6.2 in /usr/local/lib/python3.11/dist-packages (from torch) (0.6.2)\r\n",
      "Requirement already satisfied: nvidia-nccl-cu12==2.21.5 in /usr/local/lib/python3.11/dist-packages (from torch) (2.21.5)\r\n",
      "Requirement already satisfied: nvidia-nvtx-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch) (12.4.127)\r\n",
      "Collecting nvidia-nvjitlink-cu12==12.4.127 (from torch)\r\n",
      "  Downloading nvidia_nvjitlink_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\r\n",
      "Requirement already satisfied: triton==3.2.0 in /usr/local/lib/python3.11/dist-packages (from torch) (3.2.0)\r\n",
      "Requirement already satisfied: sympy==1.13.1 in /usr/local/lib/python3.11/dist-packages (from torch) (1.13.1)\r\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from sympy==1.13.1->torch) (1.3.0)\r\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.11/dist-packages (from jinja2->torch) (3.0.3)\r\n",
      "Downloading nvidia_cublas_cu12-12.4.5.8-py3-none-manylinux2014_x86_64.whl (363.4 MB)\r\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m363.4/363.4 MB\u001b[0m \u001b[31m4.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\r\n",
      "\u001b[?25hDownloading nvidia_cuda_cupti_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (13.8 MB)\r\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m13.8/13.8 MB\u001b[0m \u001b[31m58.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\r\n",
      "\u001b[?25hDownloading nvidia_cuda_nvrtc_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (24.6 MB)\r\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m24.6/24.6 MB\u001b[0m \u001b[31m52.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\r\n",
      "\u001b[?25hDownloading nvidia_cuda_runtime_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (883 kB)\r\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m883.7/883.7 kB\u001b[0m \u001b[31m47.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\r\n",
      "\u001b[?25hDownloading nvidia_cudnn_cu12-9.1.0.70-py3-none-manylinux2014_x86_64.whl (664.8 MB)\r\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m664.8/664.8 MB\u001b[0m \u001b[31m2.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\r\n",
      "\u001b[?25hDownloading nvidia_cufft_cu12-11.2.1.3-py3-none-manylinux2014_x86_64.whl (211.5 MB)\r\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m211.5/211.5 MB\u001b[0m \u001b[31m2.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\r\n",
      "\u001b[?25hDownloading nvidia_curand_cu12-10.3.5.147-py3-none-manylinux2014_x86_64.whl (56.3 MB)\r\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m56.3/56.3 MB\u001b[0m \u001b[31m28.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\r\n",
      "\u001b[?25hDownloading nvidia_cusolver_cu12-11.6.1.9-py3-none-manylinux2014_x86_64.whl (127.9 MB)\r\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m127.9/127.9 MB\u001b[0m \u001b[31m12.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\r\n",
      "\u001b[?25hDownloading nvidia_cusparse_cu12-12.3.1.170-py3-none-manylinux2014_x86_64.whl (207.5 MB)\r\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m207.5/207.5 MB\u001b[0m \u001b[31m7.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\r\n",
      "\u001b[?25hDownloading nvidia_nvjitlink_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (21.1 MB)\r\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m21.1/21.1 MB\u001b[0m \u001b[31m58.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\r\n",
      "\u001b[?25hInstalling collected packages: nvidia-nvjitlink-cu12, nvidia-curand-cu12, nvidia-cufft-cu12, nvidia-cuda-runtime-cu12, nvidia-cuda-nvrtc-cu12, nvidia-cuda-cupti-cu12, nvidia-cublas-cu12, nvidia-cusparse-cu12, nvidia-cudnn-cu12, nvidia-cusolver-cu12\r\n",
      "  Attempting uninstall: nvidia-nvjitlink-cu12\r\n",
      "    Found existing installation: nvidia-nvjitlink-cu12 12.5.82\r\n",
      "    Uninstalling nvidia-nvjitlink-cu12-12.5.82:\r\n",
      "      Successfully uninstalled nvidia-nvjitlink-cu12-12.5.82\r\n",
      "  Attempting uninstall: nvidia-curand-cu12\r\n",
      "    Found existing installation: nvidia-curand-cu12 10.3.6.82\r\n",
      "    Uninstalling nvidia-curand-cu12-10.3.6.82:\r\n",
      "      Successfully uninstalled nvidia-curand-cu12-10.3.6.82\r\n",
      "  Attempting uninstall: nvidia-cufft-cu12\r\n",
      "    Found existing installation: nvidia-cufft-cu12 11.2.3.61\r\n",
      "    Uninstalling nvidia-cufft-cu12-11.2.3.61:\r\n",
      "      Successfully uninstalled nvidia-cufft-cu12-11.2.3.61\r\n",
      "  Attempting uninstall: nvidia-cuda-runtime-cu12\r\n",
      "    Found existing installation: nvidia-cuda-runtime-cu12 12.5.82\r\n",
      "    Uninstalling nvidia-cuda-runtime-cu12-12.5.82:\r\n",
      "      Successfully uninstalled nvidia-cuda-runtime-cu12-12.5.82\r\n",
      "  Attempting uninstall: nvidia-cuda-nvrtc-cu12\r\n",
      "    Found existing installation: nvidia-cuda-nvrtc-cu12 12.5.82\r\n",
      "    Uninstalling nvidia-cuda-nvrtc-cu12-12.5.82:\r\n",
      "      Successfully uninstalled nvidia-cuda-nvrtc-cu12-12.5.82\r\n",
      "  Attempting uninstall: nvidia-cuda-cupti-cu12\r\n",
      "    Found existing installation: nvidia-cuda-cupti-cu12 12.5.82\r\n",
      "    Uninstalling nvidia-cuda-cupti-cu12-12.5.82:\r\n",
      "      Successfully uninstalled nvidia-cuda-cupti-cu12-12.5.82\r\n",
      "  Attempting uninstall: nvidia-cublas-cu12\r\n",
      "    Found existing installation: nvidia-cublas-cu12 12.5.3.2\r\n",
      "    Uninstalling nvidia-cublas-cu12-12.5.3.2:\r\n",
      "      Successfully uninstalled nvidia-cublas-cu12-12.5.3.2\r\n",
      "  Attempting uninstall: nvidia-cusparse-cu12\r\n",
      "    Found existing installation: nvidia-cusparse-cu12 12.5.1.3\r\n",
      "    Uninstalling nvidia-cusparse-cu12-12.5.1.3:\r\n",
      "      Successfully uninstalled nvidia-cusparse-cu12-12.5.1.3\r\n",
      "  Attempting uninstall: nvidia-cudnn-cu12\r\n",
      "    Found existing installation: nvidia-cudnn-cu12 9.3.0.75\r\n",
      "    Uninstalling nvidia-cudnn-cu12-9.3.0.75:\r\n",
      "      Successfully uninstalled nvidia-cudnn-cu12-9.3.0.75\r\n",
      "  Attempting uninstall: nvidia-cusolver-cu12\r\n",
      "    Found existing installation: nvidia-cusolver-cu12 11.6.3.83\r\n",
      "    Uninstalling nvidia-cusolver-cu12-11.6.3.83:\r\n",
      "      Successfully uninstalled nvidia-cusolver-cu12-11.6.3.83\r\n",
      "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\r\n",
      "libcugraph-cu12 25.6.0 requires libraft-cu12==25.6.*, but you have libraft-cu12 25.2.0 which is incompatible.\r\n",
      "pylibcugraph-cu12 25.6.0 requires pylibraft-cu12==25.6.*, but you have pylibraft-cu12 25.2.0 which is incompatible.\r\n",
      "pylibcugraph-cu12 25.6.0 requires rmm-cu12==25.6.*, but you have rmm-cu12 25.2.0 which is incompatible.\u001b[0m\u001b[31m\r\n",
      "\u001b[0mSuccessfully installed nvidia-cublas-cu12-12.4.5.8 nvidia-cuda-cupti-cu12-12.4.127 nvidia-cuda-nvrtc-cu12-12.4.127 nvidia-cuda-runtime-cu12-12.4.127 nvidia-cudnn-cu12-9.1.0.70 nvidia-cufft-cu12-11.2.1.3 nvidia-curand-cu12-10.3.5.147 nvidia-cusolver-cu12-11.6.1.9 nvidia-cusparse-cu12-12.3.1.170 nvidia-nvjitlink-cu12-12.4.127\r\n",
      "Requirement already satisfied: triton in /usr/local/lib/python3.11/dist-packages (3.2.0)\r\n"
     ]
    }
   ],
   "source": [
    "!pip install torch\n",
    "!pip install triton"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "50f56a4a",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-11-19T00:49:08.322543Z",
     "iopub.status.busy": "2025-11-19T00:49:08.321965Z",
     "iopub.status.idle": "2025-11-19T00:49:12.152910Z",
     "shell.execute_reply": "2025-11-19T00:49:12.151750Z"
    },
    "papermill": {
     "duration": 3.857994,
     "end_time": "2025-11-19T00:49:12.154914",
     "exception": false,
     "start_time": "2025-11-19T00:49:08.296920",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "import triton\n",
    "import torch\n",
    "\n",
    "def pytorch_permute_index_map(tokens, indices):\n",
    "    if indices.dim() == 1:\n",
    "        topk = 1\n",
    "    else:\n",
    "        topk = indices.size(1)\n",
    "    flatten_indices = indices.view(-1)\n",
    "    sorted_indices = torch.argsort(flatten_indices, stable=True)\n",
    "    num_out_tokens = flatten_indices.size(0)\n",
    "    permuted_tokens = tokens.index_select(0, sorted_indices[:num_out_tokens] // topk)\n",
    "    return permuted_tokens, sorted_indices\n",
    "\n",
    "\n",
    "def torch_basic(x: torch.Tensor, top_experts: torch.Tensor, tokens_per_expert: torch.Tensor, topk: int, num_experts: int):\n",
    "    block_size = 128\n",
    "    device = x.device\n",
    "    num_tokens, hidden_dim = x.shape\n",
    "\n",
    "    expert_ids_flat = top_experts.view(-1)\n",
    "\n",
    "    padded_tokens_per_expert = (\n",
    "        ((tokens_per_expert + block_size - 1) // block_size) * block_size\n",
    "    ).to(torch.int32)\n",
    "    padded_offsets = torch.cat([\n",
    "        torch.zeros(1, dtype=torch.int32, device=device),\n",
    "        padded_tokens_per_expert.cumsum(dim=0)\n",
    "    ])\n",
    "    expert_ids_cpu = expert_ids_flat.cpu().tolist()\n",
    "    padded_offsets_cpu = padded_offsets.cpu().tolist()\n",
    "\n",
    "    max_padded_tokens = padded_offsets_cpu[-1]\n",
    "    padded_tokens = torch.zeros(\n",
    "        (max_padded_tokens, hidden_dim),\n",
    "        dtype=x.dtype,\n",
    "        device=device,\n",
    "    )\n",
    "\n",
    "    assignment_groups = [[] for _ in range(num_experts)]\n",
    "    num_assignments = topk * num_tokens\n",
    "    for i in range(num_assignments):\n",
    "        expert_id = expert_ids_cpu[i]\n",
    "        assignment_groups[expert_id].append(i)\n",
    "\n",
    "    for e in range(num_experts):\n",
    "        offset = padded_offsets[e]\n",
    "        for local_idx, i in enumerate(assignment_groups[e]):\n",
    "            original_token_idx = i // topk\n",
    "            token_data = x[original_token_idx]\n",
    "            target_row = offset + local_idx\n",
    "            padded_tokens[target_row, :] = token_data\n",
    "\n",
    "    return padded_tokens, padded_tokens_per_expert"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "62359110",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-11-19T00:49:12.223042Z",
     "iopub.status.busy": "2025-11-19T00:49:12.222675Z",
     "iopub.status.idle": "2025-11-19T00:49:12.232174Z",
     "shell.execute_reply": "2025-11-19T00:49:12.231429Z"
    },
    "papermill": {
     "duration": 0.03925,
     "end_time": "2025-11-19T00:49:12.234086",
     "exception": false,
     "start_time": "2025-11-19T00:49:12.194836",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def submission(\n",
    "    x: torch.Tensor,  # (num_tokens, hidden_size) - входной тензор токенов, каждый размерности hidden_size\n",
    "    top_experts: torch.Tensor,  # (num_tokens, topk) - для каждого токена указано topk экспертов, которые он активирует\n",
    "    tokens_per_expert: torch.Tensor, # (num_experts,) - тензор размерности числа экспертов, i-ый элемент - сколько токенов приходит в i-ого эксперта\n",
    "    topk: int,  # сколько экспертов активируются на каждый токен, например, 8\n",
    "    num_experts: int,  # сколько всего экспертов в MoE, например, 128\n",
    ") -> tuple[\n",
    "    torch.Tensor,  # (max_padded, hidden_size) - padded_tokens, результат пермьюта с паддингами\n",
    "    torch.Tensor # (num_experts,) - padded_tokens_per_expert, сколько токенов приходят в каждого эксперта вместе с паддингами\n",
    "]:\n",
    "    device = x.device\n",
    "    block_size = 128\n",
    "    num_tokens, hidden_dim = x.shape\n",
    "\n",
    "    # Приводим счётчики токенов на эксперта к int64 на нужном девайсе\n",
    "    tokens_per_expert_long = tokens_per_expert.to(device=device, dtype=torch.long)\n",
    "\n",
    "    # Паддинги до кратности 128 — как в torch_basic, но в int64\n",
    "    padded_tokens_per_expert_long = (\n",
    "        (tokens_per_expert_long + (block_size - 1)) // block_size\n",
    "    ) * block_size\n",
    "\n",
    "    # Eager-permute без паддингов: токены уже отсортированы по эксперту\n",
    "    # permuted_tokens имеет длину sum(tokens_per_expert)\n",
    "    permuted_tokens, sorted_indices = pytorch_permute_index_map(x, top_experts)\n",
    "    M = permuted_tokens.size(0)\n",
    "\n",
    "    # Если по какой-то причине назначений нет\n",
    "    if M == 0:\n",
    "        total_padded = int(padded_tokens_per_expert_long.sum().item())\n",
    "        padded_tokens = torch.zeros(\n",
    "            (total_padded, hidden_dim), dtype=x.dtype, device=device\n",
    "        )\n",
    "        return padded_tokens, padded_tokens_per_expert_long.to(torch.int32)\n",
    "\n",
    "    # Для каждого assignment (строки permuted_tokens) нужен его expert_id\n",
    "    # sorted_indices — индексы assignments после сортировки\n",
    "    flatten_expert_ids = top_experts.view(-1).to(device=device, dtype=torch.long)\n",
    "    sorted_expert_ids = flatten_expert_ids.index_select(0, sorted_indices)\n",
    "\n",
    "    # Непаддинговые смещения: где начинается блок каждого эксперта\n",
    "    # start_unpadded[e] = сумма tokens_per_expert[k] по k < e\n",
    "    if num_experts > 0:\n",
    "        tpe_cumsum = tokens_per_expert_long.cumsum(dim=0)\n",
    "        unpadded_offsets = tpe_cumsum - tokens_per_expert_long\n",
    "    else:\n",
    "        unpadded_offsets = torch.zeros(0, dtype=torch.long, device=device)\n",
    "\n",
    "    # Паддинговые смещения: где начинается блок каждого эксперта в padded-тензоре\n",
    "    if num_experts > 0:\n",
    "        padded_cumsum = padded_tokens_per_expert_long.cumsum(dim=0)\n",
    "        zero = torch.zeros(1, dtype=torch.long, device=device)\n",
    "        padded_offsets_full = torch.cat([zero, padded_cumsum])  # длина = num_experts + 1\n",
    "        padded_offsets_start = padded_offsets_full[:-1]         # длина = num_experts\n",
    "        total_padded = int(padded_offsets_full[-1].item())\n",
    "    else:\n",
    "        padded_offsets_start = torch.zeros(0, dtype=torch.long, device=device)\n",
    "        total_padded = 0\n",
    "\n",
    "    # Инициализируем выходной тензор нулями (паддинги)\n",
    "    padded_tokens = torch.zeros(\n",
    "        (total_padded, hidden_dim), dtype=x.dtype, device=device\n",
    "    )\n",
    "\n",
    "    if num_experts > 0:\n",
    "        # Глобальный индекс assignment'а в permuted_tokens: 0..M-1\n",
    "        global_idx = torch.arange(M, device=device, dtype=torch.long)\n",
    "\n",
    "        # Для каждого assignment узнаём, с какого индекса начинается его эксперт\n",
    "        start_unpadded_for_assign = unpadded_offsets.index_select(0, sorted_expert_ids)\n",
    "        # Позиция этого assignment внутри блока эксперта (0,1,2,...)\n",
    "        pos_in_group = global_idx - start_unpadded_for_assign\n",
    "\n",
    "        # Стартовый индекс блока эксперта в padded-тензоре\n",
    "        start_padded_for_assign = padded_offsets_start.index_select(0, sorted_expert_ids)\n",
    "\n",
    "        # Итоговый индекс строки, куда писать этот токен\n",
    "        dst_indices = start_padded_for_assign + pos_in_group  # shape (M,)\n",
    "\n",
    "        # Векторизованный scatter: заполняем только реальные токены,\n",
    "        # остальное остаётся нулями (паддинги)\n",
    "        padded_tokens.index_copy_(0, dst_indices, permuted_tokens)\n",
    "\n",
    "    # Возвращаем padded-байтсайзы в int32, как в torch_basic\n",
    "    return padded_tokens, padded_tokens_per_expert_long.to(torch.int32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "3ebc55d6",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-11-19T00:49:12.290429Z",
     "iopub.status.busy": "2025-11-19T00:49:12.290141Z",
     "iopub.status.idle": "2025-11-19T00:53:11.400274Z",
     "shell.execute_reply": "2025-11-19T00:53:11.399381Z"
    },
    "papermill": {
     "duration": 239.16624,
     "end_time": "2025-11-19T00:53:11.429031",
     "exception": false,
     "start_time": "2025-11-19T00:49:12.262791",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Device: cuda\n",
      "\n",
      ">>> correctness quick check\n",
      "[OK]   correctness (experts=16, topk=1, hidden=256, tokens=128)\n",
      "[OK]   correctness (experts=16, topk=1, hidden=256, tokens=512)\n",
      "[OK]   correctness (experts=16, topk=1, hidden=1024, tokens=128)\n",
      "[OK]   correctness (experts=16, topk=1, hidden=1024, tokens=512)\n",
      "[OK]   correctness (experts=16, topk=4, hidden=256, tokens=128)\n",
      "[OK]   correctness (experts=16, topk=4, hidden=256, tokens=512)\n",
      "[OK]   correctness (experts=16, topk=4, hidden=1024, tokens=128)\n",
      "[OK]   correctness (experts=16, topk=4, hidden=1024, tokens=512)\n",
      "[OK]   correctness (experts=16, topk=8, hidden=256, tokens=128)\n",
      "[OK]   correctness (experts=16, topk=8, hidden=256, tokens=512)\n",
      "[OK]   correctness (experts=16, topk=8, hidden=1024, tokens=128)\n",
      "[OK]   correctness (experts=16, topk=8, hidden=1024, tokens=512)\n",
      "[OK]   correctness (experts=128, topk=1, hidden=256, tokens=128)\n",
      "[OK]   correctness (experts=128, topk=1, hidden=256, tokens=512)\n",
      "[OK]   correctness (experts=128, topk=1, hidden=1024, tokens=128)\n",
      "[OK]   correctness (experts=128, topk=1, hidden=1024, tokens=512)\n",
      "[OK]   correctness (experts=128, topk=4, hidden=256, tokens=128)\n",
      "[OK]   correctness (experts=128, topk=4, hidden=256, tokens=512)\n",
      "[OK]   correctness (experts=128, topk=4, hidden=1024, tokens=128)\n",
      "[OK]   correctness (experts=128, topk=4, hidden=1024, tokens=512)\n",
      "[OK]   correctness (experts=128, topk=8, hidden=256, tokens=128)\n",
      "[OK]   correctness (experts=128, topk=8, hidden=256, tokens=512)\n",
      "[OK]   correctness (experts=128, topk=8, hidden=1024, tokens=128)\n",
      "[OK]   correctness (experts=128, topk=8, hidden=1024, tokens=512)\n",
      "\n",
      ">>> critical case (по логам тестов: 16-4-256-128)\n",
      "\n",
      "=== bench experts=16, topk=4, hidden=256, tokens=128 ===\n",
      "ref: 21.668 ms  my: 0.421 ms  ratio=1.9%\n",
      "\n",
      ">>> grid benchmark (примерно как в тестах):\n",
      "\n",
      "=== bench experts=16, topk=1, hidden=256, tokens=128 ===\n",
      "ref: 5.732 ms  my: 0.389 ms  ratio=6.8%\n",
      "\n",
      "=== bench experts=16, topk=1, hidden=256, tokens=1024 ===\n",
      "ref: 42.817 ms  my: 0.439 ms  ratio=1.0%\n",
      "\n",
      "=== bench experts=16, topk=1, hidden=2048, tokens=128 ===\n",
      "ref: 5.716 ms  my: 0.391 ms  ratio=6.8%\n",
      "\n",
      "=== bench experts=16, topk=1, hidden=2048, tokens=1024 ===\n",
      "ref: 43.972 ms  my: 0.527 ms  ratio=1.2%\n",
      "\n",
      "=== bench experts=16, topk=1, hidden=8192, tokens=128 ===\n",
      "ref: 5.922 ms  my: 0.559 ms  ratio=9.4%\n",
      "\n",
      "=== bench experts=16, topk=1, hidden=8192, tokens=1024 ===\n",
      "ref: 43.504 ms  my: 1.342 ms  ratio=3.1%\n",
      "\n",
      "=== bench experts=16, topk=4, hidden=256, tokens=128 ===\n",
      "ref: 21.440 ms  my: 0.398 ms  ratio=1.9%\n",
      "\n",
      "=== bench experts=16, topk=4, hidden=256, tokens=1024 ===\n",
      "ref: 174.523 ms  my: 0.477 ms  ratio=0.3%\n",
      "\n",
      "=== bench experts=16, topk=4, hidden=2048, tokens=128 ===\n",
      "ref: 21.615 ms  my: 0.444 ms  ratio=2.1%\n",
      "\n",
      "=== bench experts=16, topk=4, hidden=2048, tokens=1024 ===\n",
      "ref: 170.683 ms  my: 1.304 ms  ratio=0.8%\n",
      "\n",
      "=== bench experts=16, topk=4, hidden=8192, tokens=128 ===\n",
      "ref: 21.674 ms  my: 0.866 ms  ratio=4.0%\n",
      "\n",
      "=== bench experts=16, topk=4, hidden=8192, tokens=1024 ===\n",
      "ref: 170.671 ms  my: 3.879 ms  ratio=2.3%\n",
      "\n",
      "=== bench experts=16, topk=8, hidden=256, tokens=128 ===\n",
      "ref: 42.836 ms  my: 0.401 ms  ratio=0.9%\n",
      "\n",
      "=== bench experts=16, topk=8, hidden=256, tokens=1024 ===\n",
      "ref: 342.445 ms  my: 0.632 ms  ratio=0.2%\n",
      "\n",
      "=== bench experts=16, topk=8, hidden=2048, tokens=128 ===\n",
      "ref: 42.990 ms  my: 0.521 ms  ratio=1.2%\n",
      "\n",
      "=== bench experts=16, topk=8, hidden=2048, tokens=1024 ===\n",
      "ref: 342.256 ms  my: 2.323 ms  ratio=0.7%\n",
      "\n",
      "=== bench experts=16, topk=8, hidden=8192, tokens=128 ===\n",
      "ref: 42.659 ms  my: 1.340 ms  ratio=3.1%\n",
      "\n",
      "=== bench experts=16, topk=8, hidden=8192, tokens=1024 ===\n",
      "ref: 342.020 ms  my: 7.320 ms  ratio=2.1%\n",
      "\n",
      "=== bench experts=128, topk=1, hidden=256, tokens=128 ===\n",
      "ref: 5.870 ms  my: 0.380 ms  ratio=6.5%\n",
      "\n",
      "=== bench experts=128, topk=1, hidden=256, tokens=1024 ===\n",
      "ref: 42.856 ms  my: 0.394 ms  ratio=0.9%\n",
      "\n",
      "=== bench experts=128, topk=1, hidden=2048, tokens=128 ===\n",
      "ref: 6.266 ms  my: 0.550 ms  ratio=8.8%\n",
      "\n",
      "=== bench experts=128, topk=1, hidden=2048, tokens=1024 ===\n",
      "ref: 43.920 ms  my: 1.029 ms  ratio=2.3%\n",
      "\n",
      "=== bench experts=128, topk=1, hidden=8192, tokens=128 ===\n",
      "ref: 7.574 ms  my: 1.805 ms  ratio=23.8%\n",
      "\n",
      "=== bench experts=128, topk=1, hidden=8192, tokens=1024 ===\n",
      "ref: 46.222 ms  my: 3.178 ms  ratio=6.9%\n",
      "\n",
      "=== bench experts=128, topk=4, hidden=256, tokens=128 ===\n",
      "ref: 21.936 ms  my: 0.389 ms  ratio=1.8%\n",
      "\n",
      "=== bench experts=128, topk=4, hidden=256, tokens=1024 ===\n",
      "ref: 173.698 ms  my: 0.474 ms  ratio=0.3%\n",
      "\n",
      "=== bench experts=128, topk=4, hidden=2048, tokens=128 ===\n",
      "ref: 22.384 ms  my: 0.915 ms  ratio=4.1%\n",
      "\n",
      "=== bench experts=128, topk=4, hidden=2048, tokens=1024 ===\n",
      "ref: 173.100 ms  my: 1.697 ms  ratio=1.0%\n",
      "\n",
      "=== bench experts=128, topk=4, hidden=8192, tokens=128 ===\n",
      "ref: 23.915 ms  my: 2.728 ms  ratio=11.4%\n",
      "\n",
      "=== bench experts=128, topk=4, hidden=8192, tokens=1024 ===\n",
      "ref: 173.983 ms  my: 5.599 ms  ratio=3.2%\n",
      "\n",
      "=== bench experts=128, topk=8, hidden=256, tokens=128 ===\n",
      "ref: 42.782 ms  my: 0.507 ms  ratio=1.2%\n",
      "\n",
      "=== bench experts=128, topk=8, hidden=256, tokens=1024 ===\n",
      "ref: 343.794 ms  my: 0.656 ms  ratio=0.2%\n",
      "\n",
      "=== bench experts=128, topk=8, hidden=2048, tokens=128 ===\n",
      "ref: 44.041 ms  my: 1.031 ms  ratio=2.3%\n",
      "\n",
      "=== bench experts=128, topk=8, hidden=2048, tokens=1024 ===\n",
      "ref: 344.687 ms  my: 2.647 ms  ratio=0.8%\n",
      "\n",
      "=== bench experts=128, topk=8, hidden=8192, tokens=128 ===\n",
      "ref: 44.963 ms  my: 3.205 ms  ratio=7.1%\n",
      "\n",
      "=== bench experts=128, topk=8, hidden=8192, tokens=1024 ===\n",
      "ref: 345.049 ms  my: 8.155 ms  ratio=2.4%\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "import math\n",
    "import torch\n",
    "\n",
    "torch.manual_seed(0)\n",
    "\n",
    "def make_case(num_experts, topk, hidden_size, num_tokens, device):\n",
    "    \"\"\"\n",
    "    - x: (num_tokens, hidden_size)\n",
    "    - top_experts: (num_tokens, topk), значения 0..num_experts-1\n",
    "    - tokens_per_expert: bincount по top_experts\n",
    "    \"\"\"\n",
    "    x = torch.randn(num_tokens, hidden_size, device=device)\n",
    "    top_experts = torch.randint(\n",
    "        low=0,\n",
    "        high=num_experts,\n",
    "        size=(num_tokens, topk),\n",
    "        device=device,\n",
    "    )\n",
    "    tokens_per_expert = torch.bincount(\n",
    "        top_experts.reshape(-1),\n",
    "        minlength=num_experts,\n",
    "    )\n",
    "    return x, top_experts, tokens_per_expert\n",
    "\n",
    "\n",
    "def check_correctness_once(num_experts, topk, hidden_size, num_tokens, device):\n",
    "    x, top_experts, tpe = make_case(num_experts, topk, hidden_size, num_tokens, device)\n",
    "\n",
    "    ref_out, ref_padded = torch_basic(x, top_experts, tpe, topk, num_experts)\n",
    "    my_out, my_padded = submission(x, top_experts, tpe, topk, num_experts)\n",
    "\n",
    "    if not torch.equal(ref_padded, my_padded.to(ref_padded.dtype)):\n",
    "        print(f\"[FAIL] padded_tokens_per_expert mismatch \"\n",
    "              f\"(experts={num_experts}, topk={topk}, hidden={hidden_size}, tokens={num_tokens})\")\n",
    "        print(\"ref:\", ref_padded)\n",
    "        print(\"my :\", my_padded)\n",
    "        return False\n",
    "\n",
    "    if not torch.allclose(ref_out, my_out, atol=1e-6, rtol=1e-5):\n",
    "        diff = (ref_out - my_out).abs().max().item()\n",
    "        print(f\"[FAIL] output mismatch (max diff={diff:.3e}) \"\n",
    "              f\"(experts={num_experts}, topk={topk}, hidden={hidden_size}, tokens={num_tokens})\")\n",
    "        return False\n",
    "\n",
    "    print(f\"[OK]   correctness (experts={num_experts}, topk={topk}, \"\n",
    "          f\"hidden={hidden_size}, tokens={num_tokens})\")\n",
    "    return True\n",
    "\n",
    "\n",
    "def bench_pair(fn_ref, fn_my, warmup=10, iters=50, device=\"cuda\"):\n",
    "    \"\"\"\n",
    "    (t_ref, t_my) — среднее время одного вызова в секундах.\n",
    "    \"\"\"\n",
    "    if device == \"cuda\":\n",
    "        torch.cuda.synchronize()\n",
    "\n",
    "    for _ in range(warmup):\n",
    "        fn_ref()\n",
    "        fn_my()\n",
    "    if device == \"cuda\":\n",
    "        torch.cuda.synchronize()\n",
    "\n",
    "    def run_many(fn):\n",
    "        if device == \"cuda\":\n",
    "            torch.cuda.synchronize()\n",
    "        t0 = time.perf_counter()\n",
    "        for _ in range(iters):\n",
    "            fn()\n",
    "        if device == \"cuda\":\n",
    "            torch.cuda.synchronize()\n",
    "        t1 = time.perf_counter()\n",
    "        return (t1 - t0) / iters\n",
    "\n",
    "    t_ref = run_many(fn_ref)\n",
    "    t_my = run_many(fn_my)\n",
    "    return t_ref, t_my\n",
    "\n",
    "\n",
    "def bench_single_case(num_experts, topk, hidden_size, num_tokens, device):\n",
    "    print(f\"\\n=== bench experts={num_experts}, topk={topk}, \"\n",
    "          f\"hidden={hidden_size}, tokens={num_tokens} ===\")\n",
    "    x, top_experts, tpe = make_case(num_experts, topk, hidden_size, num_tokens, device)\n",
    "\n",
    "    ref_out, ref_padded = torch_basic(x, top_experts, tpe, topk, num_experts)\n",
    "    my_out, my_padded = submission(x, top_experts, tpe, topk, num_experts)\n",
    "    assert torch.equal(ref_padded, my_padded.to(ref_padded.dtype))\n",
    "    assert torch.allclose(ref_out, my_out, atol=1e-6, rtol=1e-5)\n",
    "\n",
    "    def call_ref():\n",
    "        torch_basic(x, top_experts, tpe, topk, num_experts)\n",
    "\n",
    "    def call_my():\n",
    "        submission(x, top_experts, tpe, topk, num_experts)\n",
    "\n",
    "    t_ref, t_my = bench_pair(call_ref, call_my, device=device)\n",
    "    ratio = t_my / t_ref if t_ref > 0 else float(\"inf\")\n",
    "\n",
    "    print(f\"ref: {t_ref * 1e3:.3f} ms  my: {t_my * 1e3:.3f} ms  \"\n",
    "          f\"ratio={ratio * 100:.1f}%\")\n",
    "\n",
    "    return t_ref, t_my\n",
    "\n",
    "\n",
    "def main():\n",
    "    device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "    print(\"Device:\", device)\n",
    "\n",
    "    # 1) sanity-check\n",
    "    print(\"\\n>>> correctness quick check\")\n",
    "    for num_experts in [16, 128]:\n",
    "        for topk in [1, 4, 8]:\n",
    "            for hidden in [256, 1024]:\n",
    "                for num_tokens in [128, 512]:\n",
    "                    ok = check_correctness_once(num_experts, topk, hidden, num_tokens, device)\n",
    "                    if not ok:\n",
    "                        print(\"Корректность сломана, дальше бенчить нет смысла.\")\n",
    "                        return\n",
    "\n",
    "    # 2) `test_speed_vs_eager[16-4-256-128]`\n",
    "    print(\"\\n>>> critical case (по логам тестов: 16-4-256-128)\")\n",
    "    bench_single_case(\n",
    "        num_experts=16,\n",
    "        topk=4,\n",
    "        hidden_size=256,\n",
    "        num_tokens=128,\n",
    "        device=device,\n",
    "    )\n",
    "\n",
    "    # 3) [16/128]-[1/4/8]-[256/2048/8192]-[128/1024]\n",
    "    configs = []\n",
    "    for num_experts in [16, 128]:\n",
    "        for topk in [1, 4, 8]:\n",
    "            for hidden in [256, 2048, 8192]:\n",
    "                for num_tokens in [128, 1024]:\n",
    "                    configs.append((num_experts, topk, hidden, num_tokens))\n",
    "\n",
    "    print(\"\\n>>> grid benchmark (примерно как в тестах):\")\n",
    "    for (ne, tk, h, nt) in configs:\n",
    "        bench_single_case(ne, tk, h, nt, device=device)\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "0fff0db6",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-11-19T00:53:11.486374Z",
     "iopub.status.busy": "2025-11-19T00:53:11.486093Z",
     "iopub.status.idle": "2025-11-19T00:53:11.494385Z",
     "shell.execute_reply": "2025-11-19T00:53:11.493636Z"
    },
    "papermill": {
     "duration": 0.03852,
     "end_time": "2025-11-19T00:53:11.495641",
     "exception": false,
     "start_time": "2025-11-19T00:53:11.457121",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"\\n============================= test session starts ==============================\\nplatform linux -- Python 3.11.14, pytest-9.0.0, pluggy-1.6.0 -- /opt/conda/bin/python3.11\\ncachedir: .pytest_cache\\nhypothesis profile 'default'\\nrootdir: /workspace\\nplugins: hypothesis-6.141.0\\ncollecting ... collected 72 items\\n\\ntests.py::test_quality[16-1-256-128] PASSED                              [  1%]\\ntests.py::test_quality[16-1-256-1024] PASSED                             [  2%]\\ntests.py::test_quality[16-1-2048-128] PASSED                             [  4%]\\ntests.py::test_quality[16-1-2048-1024] PASSED                            [  5%]\\ntests.py::test_quality[16-1-8192-128] PASSED                             [  6%]\\ntests.py::test_quality[16-1-8192-1024] PASSED                            [  8%]\\ntests.py::test_quality[16-4-256-128] PASSED                              [  9%]\\ntests.py::test_quality[16-4-256-1024] PASSED                             [ 11%]\\ntests.py::test_quality[16-4-2048-128] PASSED                             [ 12%]\\ntests.py::test_quality[16-4-2048-1024] PASSED                            [ 13%]\\ntests.py::test_quality[16-4-8192-128] PASSED                             [ 15%]\\ntests.py::test_quality[16-4-8192-1024] PASSED                            [ 16%]\\ntests.py::test_quality[16-8-256-128] PASSED                              [ 18%]\\ntests.py::test_quality[16-8-256-1024] PASSED                             [ 19%]\\ntests.py::test_quality[16-8-2048-128] PASSED                             [ 20%]\\ntests.py::test_quality[16-8-2048-1024] PASSED                            [ 22%]\\ntests.py::test_quality[16-8-8192-128] PASSED                             [ 23%]\\ntests.py::test_quality[16-8-8192-1024] PASSED                            [ 25%]\\ntests.py::test_quality[128-1-256-128] PASSED                             [ 26%]\\ntests.py::test_quality[128-1-256-1024] PASSED                            [ 27%]\\ntests.py::test_quality[128-1-2048-128] PASSED                            [ 29%]\\ntests.py::test_quality[128-1-2048-1024] PASSED                           [ 30%]\\ntests.py::test_quality[128-1-8192-128] PASSED                            [ 31%]\\ntests.py::test_quality[128-1-8192-1024] PASSED                           [ 33%]\\ntests.py::test_quality[128-4-256-128] PASSED                             [ 34%]\\ntests.py::test_quality[128-4-256-1024] PASSED                            [ 36%]\\ntests.py::test_quality[128-4-2048-128] PASSED                            [ 37%]\\ntests.py::test_quality[128-4-2048-1024] PASSED                           [ 38%]\\ntests.py::test_quality[128-4-8192-128] PASSED                            [ 40%]\\ntests.py::test_quality[128-4-8192-1024] PASSED                           [ 41%]\\ntests.py::test_quality[128-8-256-128] PASSED                             [ 43%]\\ntests.py::test_quality[128-8-256-1024] PASSED                            [ 44%]\\ntests.py::test_quality[128-8-2048-128] PASSED                            [ 45%]\\ntests.py::test_quality[128-8-2048-1024] PASSED                           [ 47%]\\ntests.py::test_quality[128-8-8192-128] PASSED                            [ 48%]\\ntests.py::test_quality[128-8-8192-1024] PASSED                           [ 50%]\\ntests.py::test_speed_vs_eager[16-4-256-128] PASSED                       [ 51%]\\ntests.py::test_speed_vs_eager[16-4-256-1024] PASSED                      [ 52%]\\ntests.py::test_speed_vs_eager[16-4-256-16384] PASSED                     [ 54%]\\ntests.py::test_speed_vs_eager[16-4-2048-128] PASSED                      [ 55%]\\ntests.py::test_speed_vs_eager[16-4-2048-1024] PASSED                     [ 56%]\\ntests.py::test_speed_vs_eager[16-4-2048-16384] PASSED                    [ 58%]\\ntests.py::test_speed_vs_eager[16-4-8192-128] PASSED                      [ 59%]\\ntests.py::test_speed_vs_eager[16-4-8192-1024] PASSED                     [ 61%]\\ntests.py::test_speed_vs_eager[16-4-8192-16384] PASSED                    [ 62%]\\ntests.py::test_speed_vs_eager[16-8-256-128] PASSED                       [ 63%]\\ntests.py::test_speed_vs_eager[16-8-256-1024] PASSED                      [ 65%]\\ntests.py::test_speed_vs_eager[16-8-256-16384] PASSED                     [ 66%]\\ntests.py::test_speed_vs_eager[16-8-2048-128] PASSED                      [ 68%]\\ntests.py::test_speed_vs_eager[16-8-2048-1024] PASSED                     [ 69%]\\ntests.py::test_speed_vs_eager[16-8-2048-16384] PASSED                    [ 70%]\\ntests.py::test_speed_vs_eager[16-8-8192-128] PASSED                      [ 72%]\\ntests.py::test_speed_vs_eager[16-8-8192-1024] PASSED                     [ 73%]\\ntests.py::test_speed_vs_eager[16-8-8192-16384] PASSED                    [ 75%]\\ntests.py::test_speed_vs_eager[128-4-256-128] PASSED                      [ 76%]\\ntests.py::test_speed_vs_eager[128-4-256-1024] PASSED                     [ 77%]\\ntests.py::test_speed_vs_eager[128-4-256-16384] PASSED                    [ 79%]\\ntests.py::test_speed_vs_eager[128-4-2048-128] PASSED                     [ 80%]\\ntests.py::test_speed_vs_eager[128-4-2048-1024] PASSED                    [ 81%]\\ntests.py::test_speed_vs_eager[128-4-2048-16384] PASSED                   [ 83%]\\ntests.py::test_speed_vs_eager[128-4-8192-128] PASSED                     [ 84%]\\ntests.py::test_speed_vs_eager[128-4-8192-1024] PASSED                    [ 86%]\\ntests.py::test_speed_vs_eager[128-4-8192-16384] PASSED                   [ 87%]\\ntests.py::test_speed_vs_eager[128-8-256-128] PASSED                      [ 88%]\\ntests.py::test_speed_vs_eager[128-8-256-1024] PASSED                     [ 90%]\\ntests.py::test_speed_vs_eager[128-8-256-16384] PASSED                    [ 91%]\\ntests.py::test_speed_vs_eager[128-8-2048-128] PASSED                     [ 93%]\\ntests.py::test_speed_vs_eager[128-8-2048-1024] PASSED                    [ 94%]\\ntests.py::test_speed_vs_eager[128-8-2048-16384] PASSED                   [ 95%]\\ntests.py::test_speed_vs_eager[128-8-8192-128] PASSED                     [ 97%]\\ntests.py::test_speed_vs_eager[128-8-8192-1024] PASSED                    [ 98%]\\ntests.py::test_speed_vs_eager[128-8-8192-16384] PASSED                   [100%]\\n\\n============================= 72 passed in 49.09s ==============================\\n========= best ::test_quality and ::test_speed_vs_eager submission yet =========\\n============================= test session ends ================================\\n\""
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# LOGS / output\n",
    "\n",
    "'''\n",
    "============================= test session starts ==============================\n",
    "platform linux -- Python 3.11.14, pytest-9.0.0, pluggy-1.6.0 -- /opt/conda/bin/python3.11\n",
    "cachedir: .pytest_cache\n",
    "hypothesis profile 'default'\n",
    "rootdir: /workspace\n",
    "plugins: hypothesis-6.141.0\n",
    "collecting ... collected 72 items\n",
    "\n",
    "tests.py::test_quality[16-1-256-128] PASSED                              [  1%]\n",
    "tests.py::test_quality[16-1-256-1024] PASSED                             [  2%]\n",
    "tests.py::test_quality[16-1-2048-128] PASSED                             [  4%]\n",
    "tests.py::test_quality[16-1-2048-1024] PASSED                            [  5%]\n",
    "tests.py::test_quality[16-1-8192-128] PASSED                             [  6%]\n",
    "tests.py::test_quality[16-1-8192-1024] PASSED                            [  8%]\n",
    "tests.py::test_quality[16-4-256-128] PASSED                              [  9%]\n",
    "tests.py::test_quality[16-4-256-1024] PASSED                             [ 11%]\n",
    "tests.py::test_quality[16-4-2048-128] PASSED                             [ 12%]\n",
    "tests.py::test_quality[16-4-2048-1024] PASSED                            [ 13%]\n",
    "tests.py::test_quality[16-4-8192-128] PASSED                             [ 15%]\n",
    "tests.py::test_quality[16-4-8192-1024] PASSED                            [ 16%]\n",
    "tests.py::test_quality[16-8-256-128] PASSED                              [ 18%]\n",
    "tests.py::test_quality[16-8-256-1024] PASSED                             [ 19%]\n",
    "tests.py::test_quality[16-8-2048-128] PASSED                             [ 20%]\n",
    "tests.py::test_quality[16-8-2048-1024] PASSED                            [ 22%]\n",
    "tests.py::test_quality[16-8-8192-128] PASSED                             [ 23%]\n",
    "tests.py::test_quality[16-8-8192-1024] PASSED                            [ 25%]\n",
    "tests.py::test_quality[128-1-256-128] PASSED                             [ 26%]\n",
    "tests.py::test_quality[128-1-256-1024] PASSED                            [ 27%]\n",
    "tests.py::test_quality[128-1-2048-128] PASSED                            [ 29%]\n",
    "tests.py::test_quality[128-1-2048-1024] PASSED                           [ 30%]\n",
    "tests.py::test_quality[128-1-8192-128] PASSED                            [ 31%]\n",
    "tests.py::test_quality[128-1-8192-1024] PASSED                           [ 33%]\n",
    "tests.py::test_quality[128-4-256-128] PASSED                             [ 34%]\n",
    "tests.py::test_quality[128-4-256-1024] PASSED                            [ 36%]\n",
    "tests.py::test_quality[128-4-2048-128] PASSED                            [ 37%]\n",
    "tests.py::test_quality[128-4-2048-1024] PASSED                           [ 38%]\n",
    "tests.py::test_quality[128-4-8192-128] PASSED                            [ 40%]\n",
    "tests.py::test_quality[128-4-8192-1024] PASSED                           [ 41%]\n",
    "tests.py::test_quality[128-8-256-128] PASSED                             [ 43%]\n",
    "tests.py::test_quality[128-8-256-1024] PASSED                            [ 44%]\n",
    "tests.py::test_quality[128-8-2048-128] PASSED                            [ 45%]\n",
    "tests.py::test_quality[128-8-2048-1024] PASSED                           [ 47%]\n",
    "tests.py::test_quality[128-8-8192-128] PASSED                            [ 48%]\n",
    "tests.py::test_quality[128-8-8192-1024] PASSED                           [ 50%]\n",
    "tests.py::test_speed_vs_eager[16-4-256-128] PASSED                       [ 51%]\n",
    "tests.py::test_speed_vs_eager[16-4-256-1024] PASSED                      [ 52%]\n",
    "tests.py::test_speed_vs_eager[16-4-256-16384] PASSED                     [ 54%]\n",
    "tests.py::test_speed_vs_eager[16-4-2048-128] PASSED                      [ 55%]\n",
    "tests.py::test_speed_vs_eager[16-4-2048-1024] PASSED                     [ 56%]\n",
    "tests.py::test_speed_vs_eager[16-4-2048-16384] PASSED                    [ 58%]\n",
    "tests.py::test_speed_vs_eager[16-4-8192-128] PASSED                      [ 59%]\n",
    "tests.py::test_speed_vs_eager[16-4-8192-1024] PASSED                     [ 61%]\n",
    "tests.py::test_speed_vs_eager[16-4-8192-16384] PASSED                    [ 62%]\n",
    "tests.py::test_speed_vs_eager[16-8-256-128] PASSED                       [ 63%]\n",
    "tests.py::test_speed_vs_eager[16-8-256-1024] PASSED                      [ 65%]\n",
    "tests.py::test_speed_vs_eager[16-8-256-16384] PASSED                     [ 66%]\n",
    "tests.py::test_speed_vs_eager[16-8-2048-128] PASSED                      [ 68%]\n",
    "tests.py::test_speed_vs_eager[16-8-2048-1024] PASSED                     [ 69%]\n",
    "tests.py::test_speed_vs_eager[16-8-2048-16384] PASSED                    [ 70%]\n",
    "tests.py::test_speed_vs_eager[16-8-8192-128] PASSED                      [ 72%]\n",
    "tests.py::test_speed_vs_eager[16-8-8192-1024] PASSED                     [ 73%]\n",
    "tests.py::test_speed_vs_eager[16-8-8192-16384] PASSED                    [ 75%]\n",
    "tests.py::test_speed_vs_eager[128-4-256-128] PASSED                      [ 76%]\n",
    "tests.py::test_speed_vs_eager[128-4-256-1024] PASSED                     [ 77%]\n",
    "tests.py::test_speed_vs_eager[128-4-256-16384] PASSED                    [ 79%]\n",
    "tests.py::test_speed_vs_eager[128-4-2048-128] PASSED                     [ 80%]\n",
    "tests.py::test_speed_vs_eager[128-4-2048-1024] PASSED                    [ 81%]\n",
    "tests.py::test_speed_vs_eager[128-4-2048-16384] PASSED                   [ 83%]\n",
    "tests.py::test_speed_vs_eager[128-4-8192-128] PASSED                     [ 84%]\n",
    "tests.py::test_speed_vs_eager[128-4-8192-1024] PASSED                    [ 86%]\n",
    "tests.py::test_speed_vs_eager[128-4-8192-16384] PASSED                   [ 87%]\n",
    "tests.py::test_speed_vs_eager[128-8-256-128] PASSED                      [ 88%]\n",
    "tests.py::test_speed_vs_eager[128-8-256-1024] PASSED                     [ 90%]\n",
    "tests.py::test_speed_vs_eager[128-8-256-16384] PASSED                    [ 91%]\n",
    "tests.py::test_speed_vs_eager[128-8-2048-128] PASSED                     [ 93%]\n",
    "tests.py::test_speed_vs_eager[128-8-2048-1024] PASSED                    [ 94%]\n",
    "tests.py::test_speed_vs_eager[128-8-2048-16384] PASSED                   [ 95%]\n",
    "tests.py::test_speed_vs_eager[128-8-8192-128] PASSED                     [ 97%]\n",
    "tests.py::test_speed_vs_eager[128-8-8192-1024] PASSED                    [ 98%]\n",
    "tests.py::test_speed_vs_eager[128-8-8192-16384] PASSED                   [100%]\n",
    "\n",
    "============================= 72 passed in 49.09s ==============================\n",
    "========= best ::test_quality and ::test_speed_vs_eager submission yet =========\n",
    "============================= test session ends ================================\n",
    "'''"
   ]
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "nvidiaTeslaT4",
   "dataSources": [],
   "dockerImageVersionId": 31193,
   "isGpuEnabled": true,
   "isInternetEnabled": true,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  },
  "papermill": {
   "default_parameters": {},
   "duration": 325.466475,
   "end_time": "2025-11-19T00:53:12.440428",
   "environment_variables": {},
   "exception": null,
   "input_path": "__notebook__.ipynb",
   "output_path": "__notebook__.ipynb",
   "parameters": {},
   "start_time": "2025-11-19T00:47:46.973953",
   "version": "2.6.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
